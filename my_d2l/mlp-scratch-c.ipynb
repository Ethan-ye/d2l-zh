{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import d2lzh as d2l\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import loss as gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(nd.dot(X, W1) + b1)\n",
    "    return nd.dot(H, W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def net(X):\n",
    "#     X = X.reshape((-1, num_inputs))\n",
    "#     H1 = relu(nd.dot(X, W1) + b1)\n",
    "#     H2 = relu(nd.dot(H1, W2) + b2)\n",
    "#     return nd.dot(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))\n",
    "b1 = nd.zeros(num_hiddens)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs))\n",
    "b2 = nd.zeros(num_outputs)\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "# W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "# b1 = nd.zeros(num_hiddens1)\n",
    "# W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "# b2 = nd.zeros(num_hiddens2)\n",
    "# W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "# b3 = nd.zeros(num_outputs)\n",
    "# params = [W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "# for param in params:\n",
    "#     param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.2798, train acc 0.507, test acc 0.768\n",
      "epoch 2, loss 0.5855, train acc 0.775, test acc 0.830\n",
      "epoch 3, loss 0.4784, train acc 0.822, test acc 0.844\n",
      "epoch 4, loss 0.4247, train acc 0.843, test acc 0.853\n",
      "epoch 5, loss 0.3946, train acc 0.853, test acc 0.853\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.3728, train acc 0.862, test acc 0.870\n",
      "epoch 2, loss 0.3585, train acc 0.866, test acc 0.872\n",
      "epoch 3, loss 0.3590, train acc 0.869, test acc 0.872\n",
      "epoch 4, loss 0.3338, train acc 0.877, test acc 0.878\n",
      "epoch 5, loss 0.3219, train acc 0.880, test acc 0.875\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.8112, train acc 0.695, test acc 0.804\n",
      "epoch 2, loss 0.4894, train acc 0.819, test acc 0.844\n",
      "epoch 3, loss 0.4260, train acc 0.844, test acc 0.856\n",
      "epoch 4, loss 0.3991, train acc 0.853, test acc 0.871\n",
      "epoch 5, loss 0.3739, train acc 0.862, test acc 0.861\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.3543, train acc 0.869, test acc 0.870\n",
      "epoch 2, loss 0.3399, train acc 0.875, test acc 0.876\n",
      "epoch 3, loss 0.3281, train acc 0.879, test acc 0.871\n",
      "epoch 4, loss 0.3161, train acc 0.884, test acc 0.873\n",
      "epoch 5, loss 0.3081, train acc 0.886, test acc 0.881\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
